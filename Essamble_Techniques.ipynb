{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Essamble Techniques Questions and Answers**\n",
        "\n",
        "### 1. What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "- Ensemble Learning in machine learning is a technique that combines multiple models (often called weak learners or base models) to create a more robust, accurate, and generalized model, known as an ensemble model.\n",
        "\n",
        "> Key Idea Behind Ensemble Learning\n",
        "\n",
        "  1. A group of weak learners can come together to form a strong learner.\n",
        "\n",
        "  2. In other words, rather than relying on a single model that might be biased or limited, ensemble methods aggregate the outputs of several models to reduce errors such as bias, variance, or overfitting.\n",
        "\n",
        "> How It Works\n",
        "\n",
        "  1. Different models may make different types of errors. By combining their predictions:\n",
        "\n",
        "    a. Errors can cancel out, leading to better overall accuracy.\n",
        "\n",
        "    b. The final decision is often more stable and reliable.\n",
        "\n",
        "> Types of Ensemble Learning Techniques\n",
        "\n",
        "  1. Bagging - Train models independently in parallel on random subsets of data (with replacement). It reduces variance. Ex- Random Forest\n",
        "\n",
        "  2. Boosting - Train models sequentially, each trying to correct the errors of the previous one. It reduces bias. Ex- AdaBoost, Gradient Boosting, XGBoost\n",
        "\n",
        "  3. Stacking - Combine multiple different models using a meta-model that learns how to best combine their outputs. Ex- Stacked Generalization\n",
        "\n",
        "> Benefits of Ensemble Learning\n",
        "\n",
        "  1. Better predictive performance than any individual model\n",
        "\n",
        "  2. Reduces overfitting\n",
        "\n",
        "  3. Handles both classification and regression tasks well\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "### 2. What is the difference between Bagging and Boosting?\n",
        "\n",
        "- Bagging and Boosting are two of the most popular ensemble learning techniques, but they differ significantly in how they build and combine models.\n",
        "\n",
        "> Bagging (Bootstrap Aggregating):\n",
        "\n",
        "  1. Goal - It Reduce variance\n",
        "\n",
        "  2. Model Training - Models are trained independently and in parallel\n",
        "\n",
        "  3. Data Sampling - Uses bootstrapped datasets (random sampling with replacement)\n",
        "\n",
        "  4. Model Weighting - All models usually have equal weight\n",
        "\n",
        "  5. Overfitting - Less prone to overfitting\n",
        "\n",
        "  6. Ex- Random Forest (uses decision trees)\n",
        "\n",
        "  7. Parallelism - Can be easily parallelized\n",
        "\n",
        "  8. Think of asking multiple friends for advice independently and then taking a vote or average. The diversity in opinions smooths out extremes.\n",
        "\n",
        "  9. Suppose you're trying to predict if a loan applicant will default: Bagging builds multiple decision trees from different random subsets of your data. Each tree votes, and majority wins.\n",
        "\n",
        "\n",
        "> Boosting :\n",
        "\n",
        "  1. Goal - Reduce bias (and also variance)\n",
        "\n",
        "  2. Model Training - Models are trained sequentially, each learning from the errors of the previous\n",
        "\n",
        "  3. Data Sampling - Each new model focuses more on misclassified data points (weighted sampling)\n",
        "\n",
        "  4. Model Weighting - Models are weighted based on accuracy; better models get more influence\n",
        "\n",
        "  5. Overfitting - More prone to overfitting, but can be controlled\n",
        "\n",
        "  6. Ex- AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost\n",
        "\n",
        "  7. Parallelism - Hard to parallelize due to sequential dependency\n",
        "\n",
        "  8. Think of asking one friend for advice, then asking the next one to improve on that advice, and so on. Each friend tries to correct mistakes of the previous.\n",
        "\n",
        "  9. Suppose you're trying to predict if a loan applicant will default: Boosting builds a first tree, sees what it got wrong, gives higher weight to those wrong cases, builds a second tree to focus on them, and continues.\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "### 3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "- Bootstrap sampling is a technique where we randomly sample with replacement from the original dataset to create multiple new datasets (called bootstrap samples), each the same size as the original.\n",
        "\n",
        "> Role in Bagging (e.g., Random Forest) - In Bagging methods like Random Forest:\n",
        "\n",
        "  1. Each decision tree is trained on a different bootstrap sample.\n",
        "\n",
        "  2. Because sampling is with replacement, each tree sees a slightly different view of the data (some records may repeat, some may be left out).\n",
        "\n",
        "  3. This introduces diversity among the trees, which helps reduce variance and avoid overfitting.\n",
        "\n",
        "\n",
        "- Bootstrap sampling makes each model in the ensemble independent and diverse, which is essential for the averaging effect in Bagging to improve accuracy and stability.\n",
        "\n",
        "--\n",
        "\n",
        "\n",
        "### 4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "- Out-of-Bag (OOB) samples are the data points not included in a particular bootstrap sample when training a model in ensemble methods like Random Forest.\n",
        "\n",
        "> How OOB Samples Work:\n",
        "\n",
        "  1. In bootstrap sampling (used in bagging), about 63% of the original data is sampled with replacement to train each model.\n",
        "\n",
        "  2. The remaining ~37% that arenâ€™t selected are the OOB samples for that model.\n",
        "\n",
        "> OOB Score for Evaluation:\n",
        "\n",
        "  1. Since each data point is likely to be left out from some trees, we can use those trees to predict the outcome for that data point.\n",
        "\n",
        "  2. The OOB score is calculated by:\n",
        "\n",
        "    - Predicting each sample using only the trees that did not see it during training.\n",
        "\n",
        "    - Comparing the prediction to the actual label.\n",
        "\n",
        "    - Aggregating the accuracy over all such OOB predictions.\n",
        "\n",
        "> Why OOB Score is Useful:\n",
        "\n",
        "  1. It acts like built-in cross-validation, providing a reliable estimate of model performance without needing a separate validation set.\n",
        "\n",
        "  2. Saves time and data during model evaluation in Random Forests.\n",
        "\n",
        "- OOB score offers a quick and unbiased estimate of model accuracy using the data that each tree hasn't seen during training.\n",
        "\n",
        "--\n",
        "\n",
        "### 5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "> Single Decision Tree\n",
        "\n",
        "  1. Feature importance is calculated based on how much each feature reduces impurity (e.g., Gini or Entropy) at each split.\n",
        "\n",
        "  2. The more a feature helps split the data closer to the root, the higher its importance.\n",
        "\n",
        "  3. Drawback: It can be unstable and biased, especially if the tree is deep or overfitted.\n",
        "\n",
        "> Random Forest\n",
        "\n",
        "  1. Combines feature importance scores across all trees in the forest.\n",
        "\n",
        "  2. Importance is averaged over multiple trees, making it more stable and robust.\n",
        "\n",
        "  3. Helps reduce bias from any single tree and better handles noisy or irrelevant features.\n",
        "\n",
        "\n",
        "- Random Forest gives a more reliable and stable feature importance analysis than a single decision tree, due to its ensemble nature.\n",
        "--\n"
      ],
      "metadata": {
        "id": "JZHtukbnbGbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical Questions and Answers**"
      ],
      "metadata": {
        "id": "0Ez4R44y2ccH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#1. Write a Python program to:\n",
        "Load the Breast Cancer dataset using - sklearn.datasets.load_breast_cancer()\n",
        "Train a Random Forest Classifier\n",
        "Print the top 5 most important features based on feature importance scores.\n",
        "'''\n",
        "\n",
        "# 1. Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 2. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# 3. Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 4. Get feature importances\n",
        "feature_importances = pd.Series(model.feature_importances_, index=data.feature_names)\n",
        "\n",
        "# 5. Sort and print top 5 important features\n",
        "top_5 = feature_importances.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top_5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ9SO-WrYB7h",
        "outputId": "7ebddc19-1e14-4063-f608-2245ac6c06bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#2. Write a Python program to:\n",
        "Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "Evaluate its accuracy and compare with a single Decision Tree\n",
        "'''\n",
        "\n",
        "# 1. Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 2. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 3. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 4. Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# 5. Train a Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# 6. Print and compare accuracies\n",
        "print(f\"Decision Tree Accuracy: {dt_accuracy:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81JXr96xYOBN",
        "outputId": "cf02806d-17e5-41b9-e26c-221c8dbf028c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.00\n",
            "Bagging Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#3. Write a Python program to:\n",
        "Train a Random Forest Classifier\n",
        "Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "Print the best parameters and final accuracy\n",
        "'''\n",
        "\n",
        "# 1. Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 2. Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 3. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 4. Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [2, 4, 6, None]\n",
        "}\n",
        "\n",
        "# 5. Initialize RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 6. Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 7. Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 8. Predict and evaluate\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Final Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QQ73RIbYVvc",
        "outputId": "c90bc4fc-54e3-4e9c-87c5-95651a5286fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 2, 'n_estimators': 10}\n",
            "Final Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#4. Write a Python program to:\n",
        "Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "Compare their Mean Squared Errors (MSE)\n",
        "'''\n",
        "\n",
        "# 1. Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# 2. Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# 3. Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 4. Train a Bagging Regressor using Decision Trees\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# 5. Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "# 6. Evaluate using Mean Squared Error (MSE)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# 7. Print MSE results\n",
        "print(f\"Bagging Regressor MSE: {bagging_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACMWnV3cYfGP",
        "outputId": "32b002ae-f33a-47f4-8b7f-828cf006a907"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2579\n",
            "Random Forest Regressor MSE: 0.2577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#5. You are working as a data scientist at a financial institution to predict loan default.\n",
        "You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "1. Choose between Bagging or Boosting\n",
        "2. Handle overfitting\n",
        "3. Select base models\n",
        "4. Evaluate performance using cross-validation\n",
        "5. Justify how ensemble learning improves decision-making in this real-world context.\n",
        "'''\n",
        "\n",
        "!pip install xgboost\n",
        "\n",
        "# Real-World Ensemble Approach: Loan Default Prediction\n",
        "# Here we are working as a data scientist at a financial institution.\n",
        "# Here we want to predict loan default using ensemble learning.\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 1: Choose between Bagging or Boosting\n",
        "# ------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "For loan default prediction, the data is likely imbalanced (few defaulters),\n",
        "and capturing complex patterns is crucial.\n",
        "\n",
        "âœ… So, we prefer **Boosting** (e.g., XGBoost, Gradient Boosting) because:\n",
        "- Boosting focuses on difficult samples (like rare defaulters).\n",
        "- It reduces bias and captures complex interactions.\n",
        "- It's more suitable when model accuracy is critical.\n",
        "\n",
        "Bagging (e.g., Random Forest) is still useful for comparison or when variance reduction is preferred.\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 2: Handle Overfitting\n",
        "# ------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "To prevent overfitting:\n",
        "- Use regularization parameters in Boosting (like `max_depth`, `learning_rate`, `n_estimators`).\n",
        "- Use `early_stopping_rounds` with validation data.\n",
        "- Perform feature selection and remove noisy or irrelevant features.\n",
        "- Cross-validation helps detect overfitting early.\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 3: Select Base Models\n",
        "# ------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "For Boosting:\n",
        "- Use **Decision Trees** (shallow ones, e.g., max_depth=3) as weak learners.\n",
        "- Use **XGBoostClassifier** or **GradientBoostingClassifier** from sklearn or xgboost.\n",
        "\n",
        "For Bagging:\n",
        "- Use DecisionTreeClassifier as base estimator in BaggingClassifier or RandomForestClassifier.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Simulate dataset (as placeholder for real demographic + transaction data)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, n_clusters_per_class=2, weights=[0.85, 0.15],\n",
        "                           flip_y=0.01, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 4: Evaluate Performance with Cross-Validation\n",
        "# ------------------------------------------\n",
        "\n",
        "# Initialize Boosting model\n",
        "boost_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Cross-validation\n",
        "boost_scores = cross_val_score(boost_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Boosting CV Accuracy Scores:\", boost_scores)\n",
        "print(\"Boosting Mean CV Accuracy:\", np.mean(boost_scores))\n",
        "\n",
        "# Train and evaluate on test data\n",
        "boost_model.fit(X_train, y_train)\n",
        "y_pred = boost_model.predict(X_test)\n",
        "print(\"\\nClassification Report (Boosting):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optional: Compare with Bagging\n",
        "bag_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bag_model.fit(X_train, y_train)\n",
        "bag_pred = bag_model.predict(X_test)\n",
        "print(\"\\nClassification Report (Bagging):\")\n",
        "print(classification_report(y_test, bag_pred))\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 5: Justify Ensemble Learning in Real-World Context\n",
        "# ------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "âœ… Why Ensemble Learning for Loan Default Prediction?\n",
        "\n",
        "- In financial risk prediction, **false negatives** (predicting non-default when it is a default) can cause huge losses.\n",
        "- Boosting helps capture hard-to-predict patterns in defaulters, improving recall.\n",
        "- Ensemble models aggregate decisions from multiple learners, reducing individual bias or overfitting.\n",
        "- More robust models improve decision-making in:\n",
        "  - Credit risk scoring\n",
        "  - Customer segmentation\n",
        "  - Loan approval or rejection\n",
        "- Regulatory compliance is also supported by interpretable ensemble models like XGBoost with feature importance analysis.\n",
        "\n",
        "Conclusion:\n",
        "Ensemble learning provides more **accurate**, **stable**, and **trustworthy** predictions, making it ideal for critical financial decisions.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "ztROK7MzYtfL",
        "outputId": "ab9452ab-5b7d-4f3e-97cb-4e9807e3ba7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.0)\n",
            "Boosting CV Accuracy Scores: [0.92857143 0.89285714 0.92142857 0.93571429 0.93571429]\n",
            "Boosting Mean CV Accuracy: 0.9228571428571429\n",
            "\n",
            "Classification Report (Boosting):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.98      0.95       253\n",
            "           1       0.87      0.57      0.69        47\n",
            "\n",
            "    accuracy                           0.92       300\n",
            "   macro avg       0.90      0.78      0.82       300\n",
            "weighted avg       0.92      0.92      0.91       300\n",
            "\n",
            "\n",
            "Classification Report (Bagging):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.99      0.96       253\n",
            "           1       0.94      0.62      0.74        47\n",
            "\n",
            "    accuracy                           0.93       300\n",
            "   macro avg       0.93      0.80      0.85       300\n",
            "weighted avg       0.93      0.93      0.93       300\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nâœ… Why Ensemble Learning for Loan Default Prediction?\\n\\n- In financial risk prediction, **false negatives** (predicting non-default when it is a default) can cause huge losses.\\n- Boosting helps capture hard-to-predict patterns in defaulters, improving recall.\\n- Ensemble models aggregate decisions from multiple learners, reducing individual bias or overfitting.\\n- More robust models improve decision-making in:\\n  - Credit risk scoring\\n  - Customer segmentation\\n  - Loan approval or rejection\\n- Regulatory compliance is also supported by interpretable ensemble models like XGBoost with feature importance analysis.\\n\\nConclusion:\\nEnsemble learning provides more **accurate**, **stable**, and **trustworthy** predictions, making it ideal for critical financial decisions.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}