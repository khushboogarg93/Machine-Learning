{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Questions and Answers**\n",
        "\n",
        "### 1. What is a parameter?\n",
        "\n",
        "- In machine learning, a parameter refers to a configuration variable that is internal to the model and whose value is estimated from the training data.\n",
        "\n",
        "- Parameters are learned from data during training and are essential for making predictions.\n",
        "\n",
        "> Key Characteristics of Parameters:\n",
        "\n",
        "  1. They define the model.\n",
        "\n",
        "  2. Their values are learned automatically during training.\n",
        "\n",
        "  3. They directly affect predictions.\n",
        "\n",
        "- Ex-\n",
        "\n",
        "  1. In linear regression, the parameters are the weights (coefficients) and bias (intercept) of the line.\n",
        "\n",
        "  2. In a neural network, the parameters are the weights and biases of the neurons.\n",
        "\n",
        "  3. In decision trees, the structure of the tree is built using splits, which are not parameters but rather model structure—so decision trees are said to be parameter-free or have very few parameters.\n",
        "\n",
        "--\n",
        "\n",
        "### 2. What is correlation? What does negative correlation mean?\n",
        "\n",
        "- In machine learning, correlation refers to a statistical relationship between two variables—how one variable tends to change when another does.\n",
        "\n",
        "- Correlation measures the strength and direction of a linear relationship between two variables.\n",
        "\n",
        "- It's expressed with a value called the correlation coefficient (r): Ranges from -1 to +1, -1 (perfect -ve correlation), +1 (perfect +ve correlation), 0 (no linear correlation)\n",
        "\n",
        "> Negative Correlation Mean\n",
        "\n",
        "  - Negative correlation means that as one variable increases, the other tends to decrease i.e. they move in opposite directions.\n",
        "\n",
        "  - Suppose you're building a model to predict house prices.If the distance from the city center has a negative correlation with price, then:As distance increases, price tends to decrease.\n",
        "\n",
        "> Why is Correlation Important in Machine Learning?\n",
        "\n",
        "  1. Helps in feature selection: Highly correlated features can be redundant.\n",
        "\n",
        "  2. Can reveal multicollinearity in linear models (problematic).\n",
        "\n",
        "  3. Gives insight into data relationships before modeling.\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "### 3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "- Machine Learning (ML) is a subfield of artificial intelligence (AI) that focuses on building systems that can learn from data and make decisions or predictions without being explicitly programmed for every task.\n",
        "\n",
        "> main components in Machine Learning?\n",
        "\n",
        "  1. Data - foundation of ML, Can be structured (tables), unstructured (text, images), or semi-structured, Includes input features and (sometimes) output labels.\n",
        "\n",
        "  2. Model - mathematical structure or algorithm that maps input data to output predictions.Ex- linear regression, decision trees, neural networks.\n",
        "\n",
        "  3. Features - input variables (also called attributes or predictors).\n",
        "\n",
        "  4. Labels/Targets (for supervised learning) - output the model is supposed to predict, Used during training to learn the correct mapping.\n",
        "\n",
        "  5. Learning Algorithm - method used to adjust the model parameters based on the data.\n",
        "\n",
        "  6. Loss Function (Cost Function) - Measures the difference between the predicted output and the actual output.\n",
        "\n",
        "  7. Training Process - phase where the model learns from the data.\n",
        "\n",
        "  8. Evaluation - Measures the model's performance using metrics like accuracy, precision, recall, or RMSE, Often involves a separate test or validation dataset.\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "### 4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "- In machine learning, the loss value is a numerical measure of how well (or poorly) a model's predictions match the actual outcomes. It plays a critical role in both training and evaluating a model.\n",
        "\n",
        "> Role of Loss Value:\n",
        "\n",
        "  1. Measures Model Error - Loss function computes the difference between predicted output and true output, lower loss indicates better predictions; a higher loss indicates more error.\n",
        "\n",
        "  2. Guides the Learning Process - During training, the model uses the loss to adjust its parameters (via optimization algorithms like gradient descent). goal is to minimize the loss.\n",
        "\n",
        "  3. Indicates Model Quality (to some extent) - consistently low loss on training and validation data usually means the model is learning well.very low training loss but high validation loss may signal overfitting.\n",
        "  \n",
        "\n",
        "- loss value tells you how far off your model's predictions are from the true values. Lower loss generally means a better model—but it must be evaluated alongside other metrics (like accuracy or precision) to fully judge model performance.\n",
        "\n",
        "--\n",
        "\n",
        "### 5. What are continuous and categorical variables?\n",
        "\n",
        "- In machine learning, features (or variables) are often divided into continuous and categorical types.\n",
        "\n",
        "- Understanding the difference helps in choosing the right preprocessing methods and algorithms.\n",
        "\n",
        "> Continuous Variables\n",
        "\n",
        "  - Variables that can take any numerical value within a range (including decimals).\n",
        "\n",
        "  - Infinite possible values.\n",
        "\n",
        "  - Usually measurable quantities.\n",
        "\n",
        "  - Can be used directly or normalized/scaled.\n",
        "\n",
        "  - Often used in regression tasks.\n",
        "\n",
        "  - EX- Height = 170.5 cm, Temperature= 22.3°C, Age = 25.6 years\n",
        "\n",
        "\n",
        "> Categorical Variables\n",
        "\n",
        "  - Variables that represent categories or groups and have a finite number of distinct values.\n",
        "\n",
        "  - 2 types :\n",
        "    1. Nominal: No natural order ex- color: red, blue, green\n",
        "\n",
        "    2. Ordinal: Has an order ex- rating: low, medium, high\n",
        "\n",
        "  - Need to be encoded into numbers ex- one-hot encoding, label encoding.\n",
        "\n",
        "  - Often used in classification tasks.\n",
        "\n",
        "  - Ex- Gender (male, female)\n",
        "\n",
        "--\n",
        "\n",
        "### 6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "- Categorical variables must be converted into numerical form because most machine learning algorithms work with numbers, not text or labels. This process is called encoding.\n",
        "\n",
        "> Common Techniques for Handling Categorical Variables:\n",
        "\n",
        "  1. Label Encoding\n",
        "\n",
        "    - Assigns a unique integer to each category.\n",
        "\n",
        "    - Pros: Simple, memory-efficient\n",
        "\n",
        "    - Cons: Implies ordinal relationship (e.g., Blue > Green), which may not be true\n",
        "\n",
        "    - Best for: Tree-based models like decision trees, random forests\n",
        "\n",
        "    - Ex- Color: Red, Green, Blue → Red=0, Green=1, Blue=2\n",
        "\n",
        "  2. One-Hot Encoding\n",
        "\n",
        "    - Creates binary (0/1) columns for each category.\n",
        "\n",
        "    - Pros: No assumption of order\n",
        "\n",
        "    - Cons: Increases dimensionality (especially with many categories)\n",
        "\n",
        "    - Best for: Linear models, neural networks\n",
        "\n",
        "    - Ex- Color: Red → [1, 0, 0], Green → [0, 1, 0], Blue → [0, 0, 1]\n",
        "\n",
        "  3. Ordinal Encoding\n",
        "\n",
        "    - Like label encoding, but used only when the categories have a natural order.\n",
        "\n",
        "    - Best for: When the order matters\n",
        "\n",
        "    - Ex- Size: Small=0, Medium=1, Large=2\n",
        "\n",
        "  4. Target Encoding (Mean Encoding)\n",
        "\n",
        "    - Replaces categories with the mean of the target variable for each category.\n",
        "\n",
        "    - ex- Job: Engineer → 60K, Teacher → 40K, Doctor → 80K\n",
        "\n",
        "    - Pros: Reduces dimensionality\n",
        "\n",
        "    - Cons: Can cause data leakage if not used carefully (needs cross-validation)\n",
        "\n",
        "    - Best for: High-cardinality categorical features\n",
        "\n",
        "  5. Binary Encoding / Hash Encoding\n",
        "\n",
        "    - Converts categories to binary or hashed codes.\n",
        "\n",
        "    - Pros: Handles high cardinality better than one-hot\n",
        "\n",
        "    - Best for: Large categorical features\n",
        "\n",
        "--\n",
        "\n",
        "### 7. What do you mean by training and testing a dataset?\n",
        "\n",
        "- In machine learning, the data used to build and evaluate a model is typically divided into two main parts: the training dataset and the testing dataset.\n",
        "\n",
        "  1. Training Dataset\n",
        "\n",
        "    - Used to train the model—i.e., to learn patterns and adjust parameters.\n",
        "\n",
        "    - model sees this data during learning.\n",
        "\n",
        "    - It includes both input features and output labels (in supervised learning).\n",
        "\n",
        "    - ex- If you're predicting house prices, the training data would include house features (size, location, etc.) and known prices.\n",
        "\n",
        "  2. Testing Dataset\n",
        "\n",
        "    - Used to evaluate the model's performance on unseen data.\n",
        "\n",
        "    - model does not see this data during training.\n",
        "\n",
        "    - It simulates how the model will perform on real-world, new data.\n",
        "\n",
        "    - To check for generalization—how well the model performs beyond the training data.\n",
        "\n",
        "> Why This Split Matters:\n",
        "\n",
        "  - Prevents overfitting (model memorizing training data).\n",
        "\n",
        "  - Ensures fair evaluation on new, unseen data.\n",
        "\n",
        "  - Helps in measuring true model performance.\n",
        "\n",
        "--\n",
        "\n",
        "### 8. What is sklearn.preprocessing?\n",
        "\n",
        "- In machine learning, raw data often needs to be transformed or scaled before it can be used effectively by algorithms.\n",
        "\n",
        "- The module **sklearn.preprocessing** in Scikit-learn provides tools to preprocess this data.\n",
        "\n",
        "> Purpose of sklearn.preprocessing:\n",
        "\n",
        "  1. Prepare data for training and testing.\n",
        "\n",
        "  2. Improve model performance.\n",
        "\n",
        "  3. Ensure features are on the same scale.\n",
        "\n",
        "  4. Convert categorical data to numerical format.\n",
        "\n",
        "- sklearn.preprocessing is a toolbox for transforming raw data into a format suitable for machine learning algorithms, including scaling, encoding, and feature generation.\n",
        "\n",
        "--\n",
        "\n",
        "### 9. What is a Test set?\n",
        "\n",
        "- In machine learning, a test set is a portion of the dataset that is used to evaluate the final performance of a trained model.\n",
        "\n",
        "- It helps determine how well the model generalizes to new, unseen data.\n",
        "\n",
        "- Used only after training is complete.\n",
        "\n",
        "- The model has never seen this data during training.\n",
        "\n",
        "- Helps measure true performance on real-world data.\n",
        "\n",
        "- Contains both input features and true output labels (in supervised learning).\n",
        "\n",
        "> Why Use a Test Set?\n",
        "\n",
        "  1. To detect overfitting: a model may perform well on training data but poorly on unseen data.\n",
        "\n",
        "  2. To validate the model's effectiveness in practice.\n",
        "\n",
        "  3. To ensure that improvements in training don't just reflect memorization.\n",
        "\n",
        "- Ex- If you have 1,000 samples of data, you might split them as follows: Training Set: 800 samples → used to train the model, Test Set: 200 samples → used to test the model's accuracy\n",
        "\n",
        "- test set is a reserved portion of the dataset that is used only at the end to evaluate how well a machine learning model performs on unseen data.\n",
        "\n",
        "- It ensures the model is generalizing, not just memorizing.\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "### 10. How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?\n",
        "\n",
        "- In Python, especially with Scikit-learn, the most common way to split data is using:\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  ex- from sklearn.model_selection import train_test_split\n",
        "\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # X = features, y = labels\n",
        "\n",
        "  - test_size=0.2 → 20% of the data goes to the test set, 80% to training.\n",
        "\n",
        "  - random_state=42 → ensures reproducibility.\n",
        "\n",
        "> Approaching an ML problem involves structured thinking and step-by-step execution. Here's a typical workflow:\n",
        "\n",
        "  1. Understand the Problem\n",
        "\n",
        "  2. Collect and Explore the Data\n",
        "\n",
        "  3. Preprocess the Data - Handle missing values, Encode categorical variables, Scale or normalize numerical features, Split the data into training and testing sets.\n",
        "\n",
        "  4. Choose a Model - Select a suitable algorithm ex- Linear Regression, Decision Tree, SVM, etc. based on problem type and data size.\n",
        "\n",
        "  5. Train the Model - Fit the model using the training data (model.fit(X_train, y_train)).\n",
        "\n",
        "  6. Evaluate the Model - Use the test set (X_test, y_test) to evaluate performance. Use metrics like accuracy, precision, recall, F1-score, MSE, etc.\n",
        "\n",
        "  7. Tune the Model (Optional)\n",
        "\n",
        "  8. Deploy or Interpret Results\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "### 11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "- Exploratory Data Analysis (EDA) is the process of examining and understanding the dataset before building any machine learning model.\n",
        "\n",
        "> Key Reasons to Perform EDA:\n",
        "\n",
        "  1. Understand Data Characteristics\n",
        "\n",
        "  2. Detect Data Quality Issues\n",
        "\n",
        "  3. Feature Selection and Engineering\n",
        "\n",
        "  4. Choose Appropriate Models and Techniques\n",
        "\n",
        "  5. Inform Data Preprocessing Steps\n",
        "\n",
        "  6. Prevent Model Failures\n",
        "\n",
        "- EDA is critical because it provides a deep understanding of the data, ensures data quality, guides feature engineering, and helps in selecting and tuning the right model — all of which lead to better, more reliable machine learning outcomes.\n",
        "\n",
        "--\n",
        "\n",
        "### 12. What is correlation?\n",
        "\n",
        "- Correlation is a statistical measure that describes the strength and direction of a linear relationship between two variables.\n",
        "\n",
        "- It's expressed with a value called the correlation coefficient (r): Ranges from -1 to +1, -1 (perfect -ve correlation), +1 (perfect +ve correlation), 0 (no linear correlation)\n",
        "\n",
        "> Importance in Machine Learning:\n",
        "\n",
        "  1. Helps identify how features relate to each other and to the target variable.\n",
        "\n",
        "  2. Can be used for feature selection ex- removing redundant features.\n",
        "\n",
        "  3. Detects multicollinearity which can affect model performance.\n",
        "\n",
        "  4. Guides understanding of data patterns before modeling.\n",
        "\n",
        "- Correlation quantifies the degree to which two variables move together in a linear fashion. It is a fundamental concept to analyze relationships in data for machine learning.\n",
        "\n",
        "--\n",
        "\n",
        "### 13. What does negative correlation mean?\n",
        "\n",
        "> Negative Correlation Mean\n",
        "\n",
        "  - Negative correlation means that as one variable increases, the other tends to decrease i.e. they move in opposite directions.\n",
        "\n",
        "  - Suppose you're building a model to predict house prices.If the distance from the city center has a negative correlation with price, then:As distance increases, price tends to decrease.\n",
        "\n",
        "> Why is Correlation Important in Machine Learning?\n",
        "\n",
        "  1. Helps in feature selection: Highly correlated features can be redundant.\n",
        "\n",
        "  2. Can reveal multicollinearity in linear models (problematic).\n",
        "\n",
        "  3. Gives insight into data relationships before modeling.\n",
        "\n",
        "--\n",
        "\n",
        "### 14. How can you find correlation between variables in Python?\n",
        "\n",
        "- In machine learning and data analysis, you can find the correlation between variables using libraries like Pandas and NumPy.\n",
        "\n",
        "  1. Common Method Using Pandas:\n",
        "  \n",
        "    - Assuming you have a dataset loaded as a Pandas DataFrame called df:\n",
        "\n",
        "    - ex- # Calculate correlation matrix for all numerical columns\n",
        "          correlation_matrix = df.corr()\n",
        "          print(correlation_matrix)\n",
        "\n",
        "    - This computes the Pearson correlation coefficient between each pair of numeric variables.\n",
        "\n",
        "    - The result is a square matrix showing correlations between every pair of variables.\n",
        "\n",
        "  2. To Find Correlation Between Two Specific Variables:\n",
        "\n",
        "    - ex- correlation = df['variable1'].corr(df['variable2'])\n",
        "          print(correlation)\n",
        "\n",
        "  3. Visualizing Correlation:\n",
        "\n",
        "    ex- import seaborn as sns\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "        plt.show()\n",
        "\n",
        "- In Python, the easiest way to find correlations between variables is to use Pandas .corr() function, which computes pairwise correlation coefficients. Visualization tools like Seaborn help interpret these relationships better.\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "### 15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "- Causation (or cause-and-effect) means that one event directly causes another to happen.\n",
        "\n",
        "- In other words, a change in variable A produces a change in variable B.\n",
        "\n",
        "> correlation\n",
        "\n",
        "  - There is a positive correlation between ice cream sales and drowning incidents. Both increase in the summer, but buying ice cream does not cause drowning.\n",
        "\n",
        "  - Correlation helps in feature selection but does not guarantee that the feature causes the outcome.\n",
        "\n",
        "\n",
        "> causation\n",
        "\n",
        "  - Smoking causes an increased risk of lung cancer. Here, smoking is a direct cause affecting health outcomes.\n",
        "\n",
        "  - Inferring causation requires controlled experiments or causal inference methods, which is beyond simple correlation analysis.\n",
        "\n",
        "- Correlation means two variables move together, but causation means one actually causes the other.\n",
        "\n",
        "- Understanding this distinction is crucial to avoid misleading conclusions in machine learning and data analysis.\n",
        "\n",
        "--\n",
        "\n",
        "### 16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "- An optimizer is an algorithm or method used to adjust the parameters (like weights and biases) of a machine learning model during training to minimize the loss function.\n",
        "\n",
        "- Its goal is to find the best set of parameters that make the model predictions as accurate as possible.\n",
        "\n",
        "> Why Do We Need Optimizers?\n",
        "\n",
        "  - To minimize the error between predicted and true outputs.\n",
        "\n",
        "  - To improve model performance.\n",
        "\n",
        "  - To efficiently navigate the complex space of model parameters.\n",
        "\n",
        "> Common Types of Optimizers\n",
        "\n",
        "  1. Gradient Descent (GD) - Calculates the gradient of the loss function with respect to parameters using the entire training dataset and updates parameters in the opposite direction of the gradient.Ex- Basic linear regression or neural networks.\n",
        "\n",
        "  2. Stochastic Gradient Descent (SGD) - Updates parameters using the gradient from only one training example at a time.Ex- Large datasets, online learning.\n",
        "\n",
        "  3. Mini-batch Gradient Descent - Uses a small subset (batch) of data points to calculate the gradient and update parameters.Ex- Most deep learning frameworks use this by default.\n",
        "\n",
        "  4. Momentum - Accelerates SGD by adding a fraction of the previous update to the current one, helping to navigate flat regions or avoid oscillations. Ex- Deep learning tasks requiring faster convergence.\n",
        "\n",
        "  5. Adagrad - Adapts learning rate for each parameter individually, giving smaller updates to frequent features and larger updates to infrequent ones.Ex- Sparse data, NLP.\n",
        "\n",
        "  6. RMSprop - Modifies Adagrad by using a moving average of squared gradients to normalize the gradient, preventing the learning rate from shrinking too much. Ex- Recurrent Neural Networks (RNNs).\n",
        "\n",
        "  7. Adam (Adaptive Moment Estimation) - Combines momentum and RMSprop by keeping an exponentially decaying average of past gradients and squared gradients. Ex- Widely used in deep learning.\n",
        "\n",
        "--\n",
        "\n",
        "### 17. What is sklearn.linear_model ?\n",
        "\n",
        "- sklearn.linear_model is a module in Scikit-learn that provides linear models for both regression and classification tasks.\n",
        "\n",
        "- These models assume a linear relationship between the input features and the target variable.\n",
        "\n",
        "- It is used to fit linear models to data.\n",
        "\n",
        "- Useful for interpretable, efficient, and often surprisingly effective models, especially with high-dimensional data.\n",
        "\n",
        "- ex- Simple Linear Regression\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "- sklearn.linear_model provides a variety of linear algorithms for regression and classification, often used as baseline models or in cases where speed and interpretability are important.\n",
        "\n",
        "--\n",
        "\n",
        "### 18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "- In machine learning (especially with Scikit-learn), the .fit() method is used to train the model on the given data.\n",
        "\n",
        "- It learns the patterns in the data by adjusting the model's internal parameters (like weights).\n",
        "\n",
        "- It fits the model to the training data, meaning the model is now ready to make predictions.\n",
        "\n",
        "- Syntax = model.fit(X, y)\n",
        "\n",
        "- X: Input features (also called independent variables or predictors),y: Target variable (also called labels or dependent variable).\n",
        "\n",
        "- ex-\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)  # X_train: features, y_train: target\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "- Depending on the model, .fit() might also accept additional arguments, such as: sample_weight: to give different weights to training examples, epochs, batch_size (in deep learning frameworks like TensorFlow or Keras).\n",
        "\n",
        "- model.fit(X, y) is the step where the model learns from data by adjusting internal parameters based on the input features X and their corresponding target values y.\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "### 19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "- In machine learning, the model.predict() method is used to generate predictions from a trained model. After training the model using model.fit(), you use predict() to apply the learned patterns to new, unseen data.\n",
        "\n",
        "- It is used to make predictions on input features.\n",
        "\n",
        "- Uses the model's learned parameters to compute output values.\n",
        "\n",
        "- Syntax = model.predict(X) where X: The input data (features) for which predictions are to be made.\n",
        "\n",
        "- Ex-\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    predictions = model.predict(X_test) # Make predictions on new data\n",
        "\n",
        "- model.predict(X) takes input features and returns predicted outputs using the patterns the model has learned during training. It's the final step in most machine learning workflows.\n",
        "\n",
        "--\n",
        "\n",
        "### 20. What are continuous and categorical variables?\n",
        "\n",
        "- In machine learning, input features (also called variables) are typically classified into two main types: continuous and categorical.\n",
        "\n",
        "> Continuous Variables\n",
        "\n",
        "  - Variables that can take any numerical value within a range (including decimals).\n",
        "\n",
        "  - Infinite possible values.\n",
        "\n",
        "  - Usually measurable quantities.\n",
        "\n",
        "  - Can be used directly or normalized/scaled.\n",
        "\n",
        "  - Often used in regression tasks.\n",
        "\n",
        "  - EX- Height = 170.5 cm, Temperature= 22.3°C, Age = 25.6 years\n",
        "\n",
        "\n",
        "> Categorical Variables\n",
        "\n",
        "  - Variables that represent categories or groups and have a finite number of distinct values.\n",
        "\n",
        "  - 2 types :\n",
        "    1. Nominal: No natural order ex- color: red, blue, green\n",
        "\n",
        "    2. Ordinal: Has an order ex- rating: low, medium, high\n",
        "\n",
        "  - Need to be encoded into numbers ex- one-hot encoding, label encoding.\n",
        "\n",
        "  - Often used in classification tasks.\n",
        "\n",
        "  - Ex- Gender (male, female)\n",
        "\n",
        "--\n",
        "\n",
        "### 21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "- Feature scaling is a preprocessing technique used to normalize or standardize the range of independent variables (features) in a dataset. It ensures that all features contribute equally to the learning process.\n",
        "\n",
        "- Many machine learning algorithms are sensitive to the scale of features. If one feature has a much larger range than others, it can dominate the model's learning process, leading to biased or inaccurate results.\n",
        "\n",
        "> How Does Feature Scaling Help?\n",
        "\n",
        "  1. Speeds up gradient descent convergence.\n",
        "\n",
        "  2. Improves model accuracy by ensuring no feature dominates.\n",
        "\n",
        "  3. Helps in distance-based models like KNN and SVM where absolute values matter.\n",
        "\n",
        "  4. Makes coefficients more interpretable in linear models.\n",
        "\n",
        "- Feature scaling brings all features to the same scale, which helps many ML algorithms learn more effectively and fairly. It's a crucial step in preprocessing, especially when using algorithms sensitive to the magnitude of values.\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "### 22. How do we perform scaling in Python?\n",
        "\n",
        "- In machine learning, feature scaling is commonly performed using Scikit-learn's sklearn.preprocessing module.\n",
        "\n",
        "- It provides various scaling methods that transform features to the appropriate scale before training a model.\n",
        "\n",
        "> Common Scaling Techniques in Python:\n",
        "\n",
        "  1. Standardization (Z-score Scaling) - Transforms features to have mean = 0 and standard deviation = 1.\n",
        "\n",
        "  2. Min-Max Scaling (Normalization) - Scales features to a fixed range, usually [0, 1].\n",
        "\n",
        "  3. Robust Scaling - Uses median and interquartile range, making it less sensitive to outliers.\n",
        "\n",
        "  4. Normalizer - Scales each individual data point to unit norm (vector of length 1).\n",
        "\n",
        "> General Process for Scaling:\n",
        "\n",
        "  1. Import the scaler (e.g., StandardScaler, MinMaxScaler)\n",
        "\n",
        "  2. Fit the scaler on the training data to learn scaling parameters (mean, std, min, etc.).\n",
        "\n",
        "  3. Transform the data using the fitted scaler.\n",
        "\n",
        "  4. Use the same scaler to transform test data (to avoid data leakage).\n",
        "\n",
        "- Feature scaling in Python is typically done using Scikit-learn scalers like StandardScaler or MinMaxScaler, depending on the type and distribution of the data.\n",
        "\n",
        "--\n",
        "\n",
        "### 23. What is sklearn.preprocessing?\n",
        "\n",
        "- sklearn.preprocessing, is a module in Scikit-learn that provides tools for data preprocessing — a crucial step in any machine learning workflow.\n",
        "\n",
        "- To transform raw data into a suitable format before feeding it into a machine learning model. It includes techniques for:\n",
        "\n",
        "  1. Scaling numerical features\n",
        "\n",
        "  2. Encoding categorical variables\n",
        "\n",
        "  3. Normalizing feature values\n",
        "\n",
        "  4. Generating polynomial features\n",
        "\n",
        "  5. Handling missing or imbalanced data\n",
        "\n",
        "\n",
        "- sklearn.preprocessing is a preprocessing toolbox in Scikit-learn that helps prepare data for machine learning by scaling, encoding, transforming, or normalizing features.\n",
        "--\n",
        "\n",
        "### 24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "- In machine learning, we split the dataset into two main parts:\n",
        "\n",
        "  - Training set - used to train (fit) the model\n",
        "\n",
        "  - Testing set - used to evaluate the models performance on unseen data\n",
        "\n",
        "- This helps ensure the model generalizes well and is not overfitting.\n",
        "\n",
        "- Common Tool Used: train_test_split() from sklearn.model_selection\n",
        "\n",
        "- Required Arguments - train_test_split(X, y, test_size, random_state)\n",
        "\n",
        "- In Python, data is split for training and testing using train_test_split() from Scikit-learn, which separates your features and labels into two sets—one for model training and one for performance evaluation.\n",
        "--\n",
        "\n",
        "### 25. Explain data encoding?\n",
        "\n",
        "- Data encoding is the process of converting categorical (non-numeric) data into a numerical format so that it can be used in machine learning models, which typically require numerical input.\n",
        "\n",
        "- Most ML algorithms (like linear regression, SVM, neural networks) cannot handle non-numeric data directly.\n",
        "\n",
        "- Encoding allows the model to understand and process categorical features effectively.\n",
        "\n",
        "> Common Data Encoding Techniques:\n",
        "\n",
        "  1. Label Encoding\n",
        "  \n",
        "    - Assigns a unique integer to each category. Ex- {\"red\": 0, \"green\": 1, \"blue\": 2}\n",
        "\n",
        "    - Suitable for ordinal data (where categories have order).\n",
        "\n",
        "    - Not ideal for nominal data (unordered), as models may assume numeric order.\n",
        "\n",
        "  2. One-Hot Encoding\n",
        "\n",
        "    - Creates binary columns for each category.\n",
        "\n",
        "    - Suitable for nominal data.\n",
        "\n",
        "    - Can lead to high dimensionality if there are many unique categories.\n",
        "\n",
        "  3. Ordinal Encoding - Similar to label encoding but used for ordered categories.\n",
        "\n",
        "  4. Binary Encoding, Target Encoding\n",
        "\n",
        "    - Binary Encoding: Encodes categories as binary numbers.\n",
        "\n",
        "    - Target Encoding: Replaces each category with the average target value for that category (used in certain supervised learning tasks).\n",
        "\n",
        "- Data encoding is essential for converting categorical data into numeric form so that machine learning models can process it.\n",
        "\n",
        "- Choosing the right encoding method depends on whether the categories are ordered and how many there are.\n",
        "\n",
        "--"
      ],
      "metadata": {
        "id": "JZHtukbnbGbe"
      }
    }
  ]
}